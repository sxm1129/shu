# 章节拆分策略总结

## 核心思路

采用**优先级匹配**策略，从高优先级到低优先级依次尝试不同的章节标题模式，选择匹配数量最多的模式进行拆分。

## 策略详解

### 1. 目录（TOC）识别与处理

**目的**: 避免将目录内容误识别为章节，并利用目录信息辅助章节识别

```python
# 识别目录格式: 《书名》目录：
toc_match = re.search(r'《.*?》目录：\n(.*?)\n\n', content, re.DOTALL)

if toc_match:
    search_start_pos = toc_match.end()  # 从目录后开始搜索章节
    toc_content = toc_match.group(1)
    toc_titles = [line.strip() for line in toc_content.split('\n') if line.strip()]
    
    # 将目录中的标题作为高优先级匹配模式
    if toc_titles:
        escaped_titles = [re.escape(t) for t in toc_titles]
        title_pattern = r'(^|\n)\s*(' + '|'.join(escaped_titles) + r')\s*(\n|$)'
        high_priority_patterns.append(title_pattern)
```

**关键点**:
- 提取目录后的所有标题
- 将这些标题转换为精确匹配的正则表达式
- 设置搜索起始位置为目录结束位置，避免重复匹配

### 2. 高优先级模式（特定格式）

这些模式针对明确的章节标记格式，匹配精度高：

```python
high_priority_patterns = [
    r'(^|\n)\s*正文\s+第[0-9一二三四五六七八九十百千]+回\s+.*',  # 正文 第X回 标题
    r'(^|\n)\s*第[0-9一二三四五六七八九十百千]+回\s+.*',      # 第X回 标题
    r'(^|\n)\s*卷[0-9一二三四五六七八九十百千]+.*',          # 卷X
    r'(^|\n)\s*.*?\(\d+-.*?\)',                            # 标题(数字-副标题)
]
```

**适用场景**:
- **古典小说**: 《红楼梦》《三国演义》等使用"第X回"格式
- **历史典籍**: 使用"卷X"格式
- **现代文学**: 如《呐喊》使用"标题(编号-副标题)"格式

### 3. 低优先级模式（通用格式）

当高优先级模式无匹配时，使用这些更宽泛的模式：

```python
low_priority_patterns = [
    r'(^|\n)\s*Chapter\s+\d+.*',                          # 英文章节
    r'(^|\n)\s*[0-9一二三四五六七八九十百千]+\s*(\n|$)',  # 纯数字/中文数字
]
```

**注意**: 低优先级模式要求至少匹配2次以上才生效（`if len(matches) > 1`），避免误判。

### 4. 优先级选择逻辑

```python
# 1. 先尝试所有高优先级模式，选择匹配数最多的
for p in high_priority_patterns:
    matches = get_valid_matches(p, content, search_start_pos)
    if len(matches) > len(best_matches):
        best_matches = matches
        best_pattern = p

# 2. 如果高优先级无匹配，尝试低优先级
if not best_matches:
    for p in low_priority_patterns:
        matches = get_valid_matches(p, content, search_start_pos)
        if len(matches) > 1:  # 至少2次匹配
            if len(matches) > len(best_matches):
                best_matches = matches
                best_pattern = p
```

### 5. 章节内容提取

```python
chapters = []
if best_matches:
    for i, match in enumerate(best_matches):
        # 处理第一章前的序言/前言
        if i == 0:
            start = match.start()
            preface_start = search_start_pos
            if start > preface_start:
                preface = content[preface_start:start].strip()
                if preface:
                    chapters.append({'title': '序/前言', 'content': preface})
        
        # 提取章节标题和内容
        chapter_title = match.group().strip()
        if i < len(best_matches) - 1:
            end = best_matches[i+1].start()  # 下一章开始位置
        else:
            end = len(content)  # 最后一章到文末
        
        chapter_content = content[match.end():end].strip()
        chapters.append({'title': chapter_title, 'content': chapter_content})
else:
    # 无法识别章节，整本书作为一个章节
    chapters.append({'title': '全文', 'content': content})
```

**关键处理**:
- 自动提取第一章前的序言/前言
- 章节内容从标题结束到下一章标题开始
- 最后一章内容到文件末尾
- 无法识别时保留全文

## 使用示例

```python
def split_chapters(content):
    # 1. TOC识别
    search_start_pos = 0
    toc_match = re.search(r'《.*?》目录：\n(.*?)\n\n', content, re.DOTALL)
    if toc_match:
        search_start_pos = toc_match.end()
        # ... 处理TOC标题
    
    # 2. 定义模式
    high_priority_patterns = [...]
    low_priority_patterns = [...]
    
    # 3. 匹配选择
    best_matches = []
    for p in high_priority_patterns:
        matches = get_valid_matches(p, content, search_start_pos)
        if len(matches) > len(best_matches):
            best_matches = matches
    
    if not best_matches:
        for p in low_priority_patterns:
            matches = get_valid_matches(p, content, search_start_pos)
            if len(matches) > 1 and len(matches) > len(best_matches):
                best_matches = matches
    
    # 4. 提取章节
    chapters = []
    if best_matches:
        # ... 提取逻辑
    else:
        chapters.append({'title': '全文', 'content': content})
    
    return chapters
```

## 优化建议

### 1. 扩展性
可以根据具体书籍类型添加更多模式：
```python
# 诗集
r'(^|\n)\s*[0-9]+\.\s+.*',  # 1. 标题

# 学术论文
r'(^|\n)\s*[0-9]+(\.[0-9]+)*\s+.*',  # 1.1 标题

# 剧本
r'(^|\n)\s*第[0-9一二三四五六七八九十]+幕.*',  # 第X幕
```

### 2. 容错性
```python
def get_valid_matches(pattern, content, start_pos):
    try:
        matches = list(re.finditer(pattern, content))
    except re.error:
        return []  # 正则错误时返回空列表
    return [m for m in matches if m.start() >= start_pos]
```

### 3. 调试支持
```python
# 记录使用的模式，便于调试
if best_matches:
    print(f"使用模式: {best_pattern}")
    print(f"匹配到 {len(best_matches)} 个章节")
```

## 适用场景

✅ **适合**:
- 古典小说（章回体）
- 现代文学作品
- 有明确章节标记的文本
- 带目录的书籍

⚠️ **需要调整**:
- 诗集（需要添加诗歌特定模式）
- 散文集（可能需要按篇名识别）
- 学术论文（需要添加章节编号模式）
- 无明确章节的文本（会作为单章处理）

## 关键参数

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `search_start_pos` | 开始搜索位置 | 0（有TOC时为TOC结束位置） |
| 低优先级最小匹配数 | 避免误判 | 2 |
| 章节标题最大长度 | 数据库限制 | 255字符 |

## 总结

这个策略的核心优势：
1. **智能优先级**: 优先使用精确模式，避免误判
2. **TOC利用**: 充分利用目录信息提高准确性
3. **容错处理**: 无法识别时保留全文，不丢失内容
4. **可扩展**: 易于添加新的章节模式
5. **自动化**: 无需人工干预，批量处理大量书籍
